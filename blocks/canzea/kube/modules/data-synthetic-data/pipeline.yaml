format_version: 3
pipelines:
  ${tenant_id}-synthetic-data-${workspace}:
    group: ${tenant_id}
    environment_variables:
      PROJECT: synthetic-data
      TENANT: ${tenant_id}
    label_template: "$${source[:8]}"
    lock_behavior: none
    materials:
      source:
        git: https://github.com/ikethecoder/synthetic-data.git
        auto_update: false
        branch: master
    stages:
    - vault:
        clean_workspace: true
        elastic_profile_id: vault
        artifacts:
        - build:
            source: s3cfg
            destination: artifacts
        tasks:
        - script: |
            vault status

            export VAULT_TOKEN=$(vault write -field token auth/approle/login \
                role_id=$VAULT_ROLE_ID \
                secret_id=$VAULT_SECRET_ID)

            vault read -field data -format json secret/tenants/${tenant_id}/providers/do_s3 > s3cfg

    - read_s3:
        clean_workspace: true
        elastic_profile_id: cloud-aws
        artifacts:
        - build:
            source: outputs
            destination: artifacts
        tasks:
        - fetch:
            pipeline: ${tenant_id}-${project}-${workspace}
            stage: vault
            job: vault
            source: artifacts
            destination: .
        - script: |

            ACCESS_KEY=`cat artifacts/s3cfg | jq -r ".access_key"`
            SECRET_KEY=`cat artifacts/s3cfg | jq -r ".secret_key"`
            BUCKET=`cat artifacts/s3cfg | jq -r ".bucket"`
            RPATH="${dns_prefix}.${workspace}.ws"

            echo "
            [default]
                access_key = $ACCESS_KEY
                host_base = sfo2.digitaloceanspaces.com 
                host_bucket = %(bucket)s.sfo2.digitaloceanspaces.com
                secret_key = $SECRET_KEY
                verbosity = INFO
            " > ~/.s3cfg

            mkdir -p outputs
            s3cmd sync --acl-public s3://$BUCKET/$TENANT/$RPATH/ ./outputs/


    - build:
        clean_workspace: true
        elastic_profile_id: "ruby25"
        artifacts:
        - build:
            source: outputs
            destination: artifacts
        tasks:
        - fetch:
            pipeline: ${tenant_id}-${project}-${workspace}
            stage: read_s3
            job: read_s3
            source: artifacts
            destination: .
        - script: |
            mv artifacts/outputs outputs
            ./bin/setup
            ./bin/run
            
    - write_s3:
        clean_workspace: true
        elastic_profile_id: cloud-aws
        tasks:
        - fetch:
            pipeline: ${tenant_id}-${project}-${workspace}
            stage: vault
            job: vault
            source: artifacts
            destination: .
        - fetch:
            pipeline: ${tenant_id}-${project}-${workspace}
            stage: build
            job: build
            source: artifacts
            destination: updated
        - script: |

            ACCESS_KEY=`cat artifacts/s3cfg | jq -r ".access_key"`
            SECRET_KEY=`cat artifacts/s3cfg | jq -r ".secret_key"`
            BUCKET=`cat artifacts/s3cfg | jq -r ".bucket"`
            RPATH="${dns_prefix}.${workspace}.ws"

            echo "
            [default]
                access_key = $ACCESS_KEY
                host_base = sfo2.digitaloceanspaces.com 
                host_bucket = %(bucket)s.sfo2.digitaloceanspaces.com
                secret_key = $SECRET_KEY
                verbosity = INFO
            " > ~/.s3cfg

            (cd updated/artifacts/outputs && s3cmd sync --acl-public . s3://$BUCKET/$TENANT/$RPATH/)
